{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c66fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "Ans:\n",
    "    Min-Max scaling is a data preprocessing technique used to normalize numerical features within a specific range, typically\n",
    "    between 0 and 1. It helps to scale features with different ranges to the same scale, preventing one feature from dominating\n",
    "    the learning process over others.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "\\[X_{scaled} = \\dfrac{X - X_{min}}{X_{max} - X_{min}}\\]\n",
    "\n",
    "where \\(X_{scaled}\\) is the scaled value, \\(X\\) is the original value of the feature, \\(X_{min}\\) is the minimum value of that \n",
    "feature, and \\(X_{max}\\) is the maximum value.\n",
    "\n",
    "Example:\n",
    "Let's say we have a dataset of housing prices with a feature \"area\" representing the size of houses. The original \"area\" values \n",
    "range from 800 to 2500 square feet. To apply Min-Max scaling to this feature, we perform the following steps:\n",
    "\n",
    "1. Find the minimum and maximum values of the \"area\" feature:\n",
    "   \\(X_{min} = 800\\) (minimum value)\n",
    "   \\(X_{max} = 2500\\) (maximum value)\n",
    "\n",
    "2. Apply Min-Max scaling to a specific data point, e.g., \\(X = 1200\\) (a house with an area of 1200 square feet):\n",
    "   \\(X_{scaled} = \\dfrac{1200 - 800}{2500 - 800} = \\dfrac{400}{1700} \\approx 0.235\\)\n",
    "\n",
    "Thus, after Min-Max scaling, the \"area\" value of 1200 square feet is transformed to approximately 0.235 on the scale between 0 \n",
    "and 1. This process is repeated for all data points in the \"area\" feature, ensuring that the feature's values are within the \n",
    "desired range for effective machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d172cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "Ans:\n",
    "    The Unit Vector technique, also known as normalization or L2 normalization, is a feature scaling method used to scale numer-\n",
    "    ical features in a way that each data point (vector) has a Euclidean norm (magnitude) of 1. It rescales the feature vector \n",
    "    so that it lies on the unit hypersphere.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "\n",
    "\\[X_{unit} = \\dfrac{X}{\\|X\\|_2}\\]\n",
    "\n",
    "where \\(X_{unit}\\) is the scaled vector, \\(X\\) is the original vector, and \\(\\|X\\|_2\\) represents the L2 norm (Euclidean norm) \n",
    "of the vector, calculated as \\(\\sqrt{\\sum_{i=1}^{n} X_i^2}\\), where \\(n\\) is the number of features.\n",
    "\n",
    "Difference between Min-Max scaling and Unit Vector scaling:\n",
    "\n",
    "1. Range:\n",
    "   - Min-Max scaling rescales the features to a specific range (e.g., 0 to 1), considering the minimum and maximum values of the\n",
    "feature.\n",
    "   - Unit Vector scaling, on the other hand, doesn't restrict the values to a predefined range but scales the vectors to have a\n",
    "    unit norm.\n",
    "\n",
    "2. Effect on data distribution:\n",
    "   - Min-Max scaling preserves the original distribution of the data but limits it to a specific range.\n",
    "   - Unit Vector scaling changes the direction and magnitude of the data vectors, effectively normalizing them to have a con-\n",
    "    stant magnitude of 1 while retaining their direction.\n",
    "\n",
    "Example:\n",
    "Let's consider a dataset with two features, \"x\" and \"y\", representing coordinates of points in a 2D plane. Suppose we have a \n",
    "data point with the following original values:\n",
    "\n",
    "\\(X = [4, 3]\\)\n",
    "\n",
    "To apply Unit Vector scaling, we perform the following steps:\n",
    "\n",
    "1. Calculate the L2 norm of the vector:\n",
    "   \\(\\|X\\|_2 = \\sqrt{4^2 + 3^2} = \\sqrt{16 + 9} = \\sqrt{25} = 5\\)\n",
    "\n",
    "2. Divide each element of the vector by its L2 norm:\n",
    "   \\(X_{unit} = \\dfrac{[4, 3]}{5} = [0.8, 0.6]\\)\n",
    "\n",
    "After Unit Vector scaling, the data point [4, 3] is transformed to [0.8, 0.6], which lies on the unit hypersphere with a Eucl-\n",
    "idean norm of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "Ans:\n",
    "    PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a\n",
    "    lower-dimensional space while retaining most of its essential variation. It achieves this by identifying the principal\n",
    "    components, which are orthogonal directions that capture the maximum variance in the data.\n",
    "\n",
    "Steps in PCA:\n",
    "\n",
    "1. Mean Centering: Subtract the mean from each feature to center the data around the origin.\n",
    "\n",
    "2. Covariance Matrix: Compute the covariance matrix to understand the relationships between different features.\n",
    "\n",
    "3. Eigendecomposition: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal \n",
    "    components, and eigenvalues indicate the amount of variance captured by each component.\n",
    "\n",
    "4. Select Components: Order the eigenvectors by their corresponding eigenvalues in descending order. Select the top k eigenve-\n",
    "    ctors to retain the most important information while reducing dimensions.\n",
    "\n",
    "5. Projection: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with two features, \"x\" and \"y,\" representing the performance of students on two exams. The data is in a 2D \n",
    "space.\n",
    "\n",
    "Original Data Points:\n",
    "```\n",
    "[70, 75]\n",
    "[85, 82]\n",
    "[60, 65]\n",
    "[90, 88]\n",
    "[75, 78]\n",
    "```\n",
    "\n",
    "Steps in PCA:\n",
    "\n",
    "1. Mean Centering:\n",
    "```\n",
    "[70-76.0, 75-77.6] = [-6.0, -2.6]\n",
    "[85-76.0, 82-77.6] = [9.0, 4.4]\n",
    "[60-76.0, 65-77.6] = [-16.0, -12.6]\n",
    "[90-76.0, 88-77.6] = [14.0, 10.4]\n",
    "[75-76.0, 78-77.6] = [-1.0, 0.4]\n",
    "```\n",
    "\n",
    "2. Covariance Matrix:\n",
    "```\n",
    "Covariance Matrix = | 72.0  58.8 |\n",
    "                    | 58.8  49.2 |\n",
    "```\n",
    "\n",
    "3. Eigendecomposition:\n",
    "The eigenvectors and eigenvalues for the covariance matrix are calculated.\n",
    "\n",
    "4. Select Components:\n",
    "Suppose we choose to retain only one principal component. After ordering by eigenvalues in descending order, we select the top \n",
    "eigenvector: \\([0.88, 0.47]\\).\n",
    "\n",
    "5. Projection:\n",
    "Projecting the mean-centered data onto the selected principal component:\n",
    "```\n",
    "[[-6.0, -2.6] dot [0.88, 0.47] = -6.7\n",
    " [9.0, 4.4]   dot [0.88, 0.47] = 10.4\n",
    " [-16.0, -12.6] dot [0.88, 0.47] = -18.4\n",
    " [14.0, 10.4] dot [0.88, 0.47] = 17.6\n",
    " [-1.0, 0.4]   dot [0.88, 0.47] = 0.3\n",
    "```\n",
    "\n",
    "The lower-dimensional representation of the data using PCA, retaining one principal component, would be:\n",
    "```\n",
    "[-6.7]\n",
    "[10.4]\n",
    "[-18.4]\n",
    "[17.6]\n",
    "[0.3]\n",
    "```\n",
    "\n",
    "This way, PCA has reduced the dimensionality of the data from 2D to 1D while preserving the most important information\n",
    " (variance) along the chosen principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "Ans:\n",
    "    PCA is a feature extraction technique used to transform high-dimensional data into a lower-dimensional space while preser-\n",
    "    ving as much of the essential variation as possible. It achieves this by identifying the principal components, which are \n",
    "    linear combinations of the original features that capture the maximum variance in the data. These principal components can \n",
    "    be seen as new features that represent the most important information in the data.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with five features: \"A,\" \"B,\" \"C,\" \"D,\" and \"E,\" representing various characteristics of different products. \n",
    "    The dataset has a high dimensionality (i.e., many features).\n",
    "\n",
    "Original Data:\n",
    "```\n",
    "Product | A  | B  | C  | D  | E\n",
    "-------------------------------\n",
    "Prod1   | 3  | 5  | 7  | 2  | 8\n",
    "Prod2   | 4  | 6  | 8  | 3  | 9\n",
    "Prod3   | 2  | 4  | 6  | 1  | 7\n",
    "Prod4   | 5  | 7  | 9  | 4  | 10\n",
    "Prod5   | 1  | 3  | 5  | 1  | 6\n",
    "```\n",
    "\n",
    "To use PCA for feature extraction:\n",
    "\n",
    "1. Mean Centering: Subtract the mean from each feature to center the data around the origin.\n",
    "\n",
    "2. Covariance Matrix: Compute the covariance matrix to understand the relationships between different features.\n",
    "\n",
    "3. Eigendecomposition: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal\n",
    "    components, and eigenvalues indicate the amount of variance captured by each component.\n",
    "\n",
    "4. Select Components: Order the eigenvectors by their corresponding eigenvalues in descending order. Select the top k eigenve-\n",
    "    ctors to retain the most important information while reducing dimensions.\n",
    "\n",
    "5. Projection: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Suppose we want to reduce the dimensionality to two dimensions (k=2) using PCA:\n",
    "\n",
    "1. Mean Centering: Center the data.\n",
    "\n",
    "2. Covariance Matrix:\n",
    "```\n",
    "Covariance Matrix = | 2.5   4.5   6.5   2.0   8.5 |\n",
    "                    | 4.5   8.5   12.5  4.0   17.5|\n",
    "                    | 6.5   12.5  18.5  6.0   26.5|\n",
    "                    | 2.0   4.0   6.0   2.0   8.0 |\n",
    "                    | 8.5   17.5  26.5  8.0   35.5|\n",
    "```\n",
    "\n",
    "3. Eigendecomposition:\n",
    "The eigenvectors and eigenvalues for the covariance matrix are calculated.\n",
    "\n",
    "4. Select Components:\n",
    "Suppose the top two eigenvectors are: \\([0.281, 0.566, 0.781, 0.25, 0.938]\\) and \\([0.728, 0.049, -0.649, -0.098, 0.211]\\).\n",
    "\n",
    "5. Projection:\n",
    "Project the data onto the two selected principal components:\n",
    "```\n",
    "[Prod1, Prod2, Prod3, Prod4, Prod5] dot [0.281, 0.566] = [6.139, 9.557, 3.619, 12.443, 2.614]\n",
    "                                         dot [0.728, 0.049] = [3.012, 5.997, 2.008, 7.44, 1.106]\n",
    "```\n",
    "\n",
    "The reduced two-dimensional representation of the data using PCA would be:\n",
    "```\n",
    "[6.139, 3.012]\n",
    "[9.557, 5.997]\n",
    "[3.619, 2.008]\n",
    "[12.443, 7.44]\n",
    "[2.614, 1.106]\n",
    "```\n",
    "\n",
    "This way, PCA has extracted two principal components, representing the most significant information in the original data, effe-\n",
    "ctively reducing the dimensionality from five features to two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "Ans:\n",
    "    To use Min-Max scaling for preprocessing the data in the food delivery service recommendation system:\n",
    "\n",
    "1. Identify the features: The dataset contains various features such as price, rating, and delivery time that need to be proce-\n",
    "    ssed.\n",
    "\n",
    "2. Compute the minimum and maximum values: Calculate the minimum and maximum values for each feature to determine the scaling \n",
    "    range. For example, find the minimum and maximum values for the price, rating, and delivery time features.\n",
    "\n",
    "3. Apply Min-Max scaling: For each data point in the dataset, apply the Min-Max scaling formula to normalize the values of each \n",
    "    feature within the specified range (usually between 0 and 1).\n",
    "\n",
    "The Min-Max scaling formula is:\n",
    "\\[X_{scaled} = \\dfrac{X - X_{min}}{X_{max} - X_{min}}\\]\n",
    "\n",
    "where \\(X\\) is the original value of the feature, \\(X_{min}\\) is the minimum value of that feature, and \\(X_{max}\\) is the max-\n",
    "imum value of that feature.\n",
    "\n",
    "4. Updated dataset: After applying Min-Max scaling to all relevant features (price, rating, delivery time, etc.), you will have\n",
    "    a preprocessed dataset where all the features are scaled within the range of 0 to 1.\n",
    "\n",
    "This preprocessing step is crucial for building the recommendation system because it ensures that the different features are br-\n",
    "ought to a similar scale. This prevents any one feature from dominating the recommendation process over others due to differen-\n",
    "ces in their scales. Having all features on a common scale also allows the recommendation algorithm to weigh the features equal-\n",
    "ly and make accurate and balanced recommendations to the users of the food delivery service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "Ans:\n",
    "    To use PCA for reducing the dimensionality of the dataset in the stock price prediction project:\n",
    "\n",
    "1. Identify the features: The dataset contains numerous features, such as company financial data and market trends, contributing\n",
    "    to the high dimensionality.\n",
    "\n",
    "2. Standardize the data: Before applying PCA, it's essential to standardize the data to have zero mean and unit variance. This \n",
    "    step is necessary to ensure that features with large variances do not dominate the principal component selection process.\n",
    "\n",
    "3. Perform PCA: Apply PCA on the standardized dataset to identify the principal components that capture the most significant va-\n",
    "    riance in the data. PCA will compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4. Select the number of components: Determine the number of principal components to retain. This decision can be based on the \n",
    "    explained variance ratio or the cumulative sum of eigenvalues. A common practice is to select the top k components that \n",
    "    explain a high percentage of the total variance, effectively reducing the dimensionality to a lower value k.\n",
    "\n",
    "5. Project the data: Transform the original dataset using the selected k principal components. This projection results in a \n",
    "    lower-dimensional dataset with reduced features while still capturing the essential information.\n",
    "\n",
    "6. Train the prediction model: Utilize the reduced dataset as input for training the stock price prediction model. The reduced \n",
    "    feature set will speed up the training process and may also help in avoiding overfitting.\n",
    "\n",
    "By using PCA to reduce dimensionality, the stock price prediction model will benefit from the reduced computational complexity,\n",
    "improved generalization, and potentially a more interpretable feature set. The transformed dataset, containing a reduced number\n",
    "of principal components, will allow the model to focus on the most important aspects of the data and make more efficient predi-\n",
    "ctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "Ans:\n",
    "    To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1:\n",
    "\n",
    "1. Find the minimum and maximum values in the dataset:\n",
    "   Minimum value (X_min) = 1\n",
    "   Maximum value (X_max) = 20\n",
    "\n",
    "2. Apply Min-Max scaling using the formula:\n",
    "   \\[X_{scaled} = \\dfrac{X - X_{min}}{X_{max} - X_{min}}\\]\n",
    "\n",
    "3. Scale each value in the dataset:\n",
    "\n",
    "   For X = 1:\n",
    "   \\[X_{scaled} = \\dfrac{1 - 1}{20 - 1} = 0\\]\n",
    "\n",
    "   For X = 5:\n",
    "   \\[X_{scaled} = \\dfrac{5 - 1}{20 - 1} = \\dfrac{4}{19} \\approx 0.211\\]\n",
    "\n",
    "   For X = 10:\n",
    "   \\[X_{scaled} = \\dfrac{10 - 1}{20 - 1} = \\dfrac{9}{19} \\approx 0.474\\]\n",
    "\n",
    "   For X = 15:\n",
    "   \\[X_{scaled} = \\dfrac{15 - 1}{20 - 1} = \\dfrac{14}{19} \\approx 0.737\\]\n",
    "\n",
    "   For X = 20:\n",
    "   \\[X_{scaled} = \\dfrac{20 - 1}{20 - 1} = 1\\]\n",
    "\n",
    "The scaled values of the dataset [1, 5, 10, 15, 20] within the range of -1 to 1 are approximately:\n",
    "\n",
    "\\[-1, 0.211, 0.474, 0.737, 1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "Ans:\n",
    "    To perform Feature Extraction using PCA on the dataset with features: [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "1. Standardize the data: Before applying PCA, standardize the dataset to have zero mean and unit variance for each feature. This\n",
    "    step ensures that all features contribute equally to the principal component selection.\n",
    "\n",
    "2. Perform PCA: Apply PCA on the standardized dataset to compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "3. Select the number of principal components: Determine the number of principal components to retain. This decision can be based\n",
    "    on the explained variance ratio or the cumulative sum of eigenvalues.\n",
    "\n",
    "4. Reasoning for choosing the number of principal components: There is no specific rule to determine the exact number of princ-\n",
    "    ipal components to retain, as it depends on the desired trade-off between dimensionality reduction and information preserv-ation. However, a common approach is to select the number of principal components that explain a high percentage (e.g., 95% or 99%) of the total variance in the data.\n",
    "\n",
    "For example, if the cumulative sum of eigenvalues indicates that the first three principal components explain 95% of the variance, it may be reasonable to choose to retain these three components. This decision reduces the dimensionality significantly while retaining most of the essential information in the data.\n",
    "\n",
    "The number of principal components to retain should be chosen carefully, considering factors such as the desired level of dimensionality reduction, computational efficiency, and the impact on the predictive performance of the downstream model that will use the reduced feature set.\n",
    "\n",
    "Note: The actual variance explained by each principal component can be visualized using a scree plot or cumulative variance plot to aid in the decision-making process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
